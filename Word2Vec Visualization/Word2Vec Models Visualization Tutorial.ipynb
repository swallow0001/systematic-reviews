{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model Tensorboard Visualization\n",
    "\n",
    "Tutorial | by Qixiang Fang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial on how to easily visualize and thus interpret your word2vec models on the __tensorboard projector__. We also demonstrate via such visualizations that __our approach of training a data set on a model already built on the FastText wikipedia pre-trained word vector data__ achieves better performance, in comparison to either models trained on just a new data set or the wikepedia pre-trained data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from gensim.scripts import word2vec2tensor\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simple function to pre-process data usable for gensim\n",
    "def process_data(file):\n",
    "    for line in file:\n",
    "        yield gensim.utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A re-written gensim function to convert word2vec files to tensor formates, \n",
    "#due to decoding procedures incompatible with python 35\n",
    "def word2vec2tensor2(word2vec_model_path, tensor_filename, binary=False):\n",
    "\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=False)\n",
    "    outfiletsv = tensor_filename + '_tensor.tsv'\n",
    "    outfiletsvmeta = tensor_filename + '_metadata.tsv'\n",
    "    \n",
    "    with open(outfiletsv, 'w+') as file_vector:\n",
    "        with open(outfiletsvmeta, 'w+') as file_metadata:\n",
    "            for word in model.index2word:\n",
    "                try:\n",
    "                    file_metadata.write(gensim.utils.to_utf8(word).decode(\"utf-8\") + gensim.utils.to_utf8('\\n').decode(\"utf-8\"))\n",
    "                    vector_row = '\\t'.join(str(x) for x in model[word])\n",
    "                    file_vector.write(vector_row + '\\n')\n",
    "                except UnicodeEncodeError:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simple function to get current time in string formats, used later for file names\n",
    "def get_time():\n",
    "    now = datetime.datetime.now()\n",
    "    time = now.strftime(\"%Y-%m-%d %H-%M\")\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-444e709751a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Set your working directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\Law_Adder\\\\Desktop\\\\word2vec\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#just my own example\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Import data (the ptsd data set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'schoot_train_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#Set your working directory\n",
    "os.chdir(\"C:\\\\Users\\\\Law_Adder\\\\Desktop\\\\word2vec\") #just my own example\n",
    "\n",
    "#Import data (the ptsd data set)\n",
    "data = pd.read_csv('schoot_train_data.csv')\n",
    "\n",
    "#Merge individual titles and abstracts into single strings\n",
    "usedata = data['title'].fillna('') + ' ' + data['abstract'].fillna('')\n",
    "\n",
    "#Convert from panda data frame to lists\n",
    "usedata_list = usedata.values.tolist() \n",
    "\n",
    "#Preprocess data into a format workable with gensim\n",
    "usedata_gen = process_data(usedata_list)\n",
    "usedata_clean = list(usedata_gen) #This is the data we will be working with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Facebook FastText files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section details how you can convert FastText word2vec files into gensim word2vec files which gensim can work with. \n",
    "\n",
    "You can download the FastText word2vec files from https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "Please use the _'text'_ file instead of _'bin + text'_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Load the original Facebook FastText files. This can take a while!!!\n",
    "en_model = KeyedVectors.load_word2vec_format('wiki.en.vec')\n",
    "#Convert and save as gensim word2vec models\n",
    "en_model.save_word2vec_format(\"wiki\"+\".bin\", binary=True)\n",
    "del en_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Just PTSD Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a word2vec model based on only the PTST data set\n",
    "#Only includes words with a frequency of at least 10\n",
    "model1 = Word2Vec(usedata_clean, size = 300, min_count=10)\n",
    "\n",
    "#Check words similar to 'ptsd'\n",
    "model1.most_similar(positive = \"ptsd\")\n",
    "\n",
    "#Save model as word2vec format\n",
    "model1_name = \"word2vec_ptsd\" + get_time()\n",
    "model1.wv.save_word2vec_format(model1_name + \".txt\", binary=False)\n",
    "\n",
    "#Convert word2vec file to tensorboard format\n",
    "word2vec2tensor2(model1_name + \".txt\", model1_name, binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Just FastText Pre-trained Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare a word2vec model with a vocabulary identital to the one in Model 1\n",
    "model2 = Word2Vec(size=300, min_count=10)\n",
    "model2.build_vocab(usedata_clean)\n",
    "len(model2.wv.vocab) #Check vocabulary size\n",
    "\n",
    "#Load the FastText data file into the model, train it, but only for words already\n",
    "#defined in the vocabulary, and set lockf = 1 so that word vectors can be updated\n",
    "model2.intersect_word2vec_format(fname = \"wiki.bin\", binary = True, lockf = 1)\n",
    "\n",
    "#Check words similar to 'ptsd'\n",
    "model2.most_similar(positive = \"ptsd\")\n",
    "\n",
    "#Save model as word2vec format\n",
    "model2_name = \"word2vec_wiki\" + get_time()\n",
    "model2.wv.save_word2vec_format(model2_name + \".txt\", binary=False)\n",
    "\n",
    "#Convert word2vec file to tensorboard format\n",
    "word2vec2tensor2(model2_name + \".txt\", model2_name, binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: PTST + FastText/Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reuse model 2 for model 3\n",
    "model3 = model2\n",
    "\n",
    "#Continue training model 2 with the PTSD data set\n",
    "model3.train(usedata_clean, total_examples=len(usedata_clean), epochs = 10)\n",
    "\n",
    "#Check words similar to 'ptsd'\n",
    "model3.most_similar(positive = \"ptsd\")\n",
    "\n",
    "#Save model as word2vec format\n",
    "model3_name = \"word2vec_both\" + get_time()\n",
    "model3.wv.save_word2vec_format(model3_name + \".txt\", binary=False)\n",
    "\n",
    "#Convert word2vec file to tensorboard format\n",
    "word2vec2tensor2(model3_name + \".txt\", model3_name, binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe how these three models relate to each other by checking word associations. For instance, we have done so by finding words similar to '__ptsd__'. You can see that the results become more meaningful and interpretable over the three models. \n",
    "\n",
    "But what is more interesting is to visualize the results using the tensorboard files (tsv. formats) we generated for each model.\n",
    "\n",
    "1. Use this website: https://projector.tensorflow.org/\n",
    "\n",
    "2. Upload your data: both __metadata__ and __vector__ files\n",
    "\n",
    "3. Click anywhere outside the box where you upload your data to proceed\n",
    "\n",
    "4. Activate regular expression (__.\\*__) for the search bar on the lefthand panel\n",
    "\n",
    "5. Search keywords: Use '__\\b__' as a container for an exact match of keywords. For instance, __\\bptsd\\b__, where you can also see semantically closest words for '__ptsd__'.\n",
    "\n",
    "6. Use \"__|__\" (OR operator) to show multiple keywords at the same time, such as 'ptsd' and 'stress': __\\bptsd\\b|\\bstress\\b__. Don't include space.\n",
    "\n",
    "Across models (i.e. FastTest vs. PTSD Data Set vs. FastTest+PTSD), you can see for instance the relationship between such a word pair improves or worsens. For the word pair '__ptsd vs. stress__', we can see that these two words become spatially closer to each other over the three models, suggesting that using the FastText wikipedia data set as the first embedding layer in a word2vec model improves performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
