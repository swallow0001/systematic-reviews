{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR - An experimental notebook on supervised Neural Networks.\n",
    "\n",
    "\n",
    "## Ideas\n",
    "\n",
    "- symantic web technology, micro applications (combine the approaches)\n",
    "- the role of authors, citations\n",
    "- sensitivity 1 doesn't work. (multiple raters, different expertise)\n",
    "- citation networks\n",
    "- use of rules how to interpreted the text. \n",
    "\n",
    "\n",
    "## Remarks\n",
    "\n",
    "- Covidence and Rayyan (software)\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "# Systematic review\n",
    "\n",
    "Based on: https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "\n",
    "> This script loads pre-trained word embeddings (GloVe embeddings)\n",
    "into a frozen Keras Embedding layer, and uses it to\n",
    "train a text classification model on the 20 Newsgroup dataset\n",
    "(classification of newsgroup messages into 20 different categories).\n",
    "GloVe embedding data can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "20 Newsgroup data can be found at:\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "- minimize number of papers\n",
    "- max # of fn =1\n",
    "- threshhold >0\n",
    "\n",
    "\n",
    "## Related links\n",
    "https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\n",
    "https://www.quora.com/What-deep-learning-method-to-use-to-classify-text-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# CPython dependencies\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# external dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras_metrics\n",
    "from nltk.corpus import stopwords\n",
    "from prettytable import PrettyTable\n",
    "from numpy import cumsum\n",
    "from matplotlib import pyplot\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# keras dependencies\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# project dependencies\n",
    "sys.path.insert(0, os.path.join('..', 'python'))\n",
    "\n",
    "from utils import load_ptsd_data\n",
    "from config import LOG_DIR, OUTPUT_DIR, DATA_DIR, GLOVE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Basic data related variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target variable\n",
    "TARGET_VARIABLE = \"included_final\" # \"included_ats\" (after title screening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = load_ptsd_data()\n",
    "print('{} articles in SR, {} included'.format(len(texts), sum(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenizing\n",
    "\n",
    "Apply vectorization to texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# get the word index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, \n",
    "                     maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding='post', truncating='post')\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation set\n",
    "Split the dataset into a training and a validation set. Use SciKit-learn for this. \n",
    "We decided to train our model on a small dataset (around 500), to make the environment more simulated to the active learning approach where we have small number of labeled data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(validation_split, added_positives):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        data,\n",
    "        labels,\n",
    "        test_size=validation_split,\n",
    "        random_state=2018,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    # add added_positives positive paper to training dataset\n",
    "    if added_positives>0 :\n",
    "        positive_indx = np.where(y_val[:,1]==1)[0]\n",
    "        x_train = np.vstack((x_train, x_val[positive_indx[0:added_positives]]))\n",
    "        y_train = np.vstack((y_train,y_val[positive_indx[0:added_positives]]))\n",
    "\n",
    "        x_val = np.delete(x_val, positive_indx[0:added_positives],0)\n",
    "        y_val = np.delete(y_val, positive_indx[0:added_positives],0)\n",
    "\n",
    "    return (x_train, x_val, y_train, y_val)\n",
    "\n",
    "x_train, x_val, y_train, y_val = split_data(0.2, 0)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up an embedding layer\n",
    "\n",
    "We make use of a 300 dimensional embedding layer for the RNN models. This embedding layer is based on pretrained Word2Vec models. In this step we match our vocabulary with the vocabulary of the pretrained models. Thereafter, we embed the result in a Keras Embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "# set dimensions\n",
    "EMBEDDING_DIM = 300\n",
    "pretrained_word2vec_fp = os.path.join(GLOVE_DIR, 'wiki.en.vec')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(pretrained_word2vec_fp, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        values = line.split()\n",
    "        split_on_i = len(values) - EMBEDDING_DIM\n",
    "        word = ' '.join(values[0:split_on_i])\n",
    "        coefs = np.asarray(values[split_on_i:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        \n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "                \n",
    "print('Shape of embedding matrix: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is use to create a embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "def build_embedding(weights):\n",
    "    return Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[weights],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "embedding_layer = build_embedding(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "This section contains models to use later on. Focusing on Convolutional and LSTM models. \n",
    "\n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(backwards, dropout, neurons,optimizer ):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = LSTM(neurons,input_shape=(MAX_SEQUENCE_LENGTH,),  go_backwards=backwards, dropout=dropout)(embedded_sequences)\n",
    "    x = Dense(128,activation='relu')(x)\n",
    "    output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model_lstm = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "    model_lstm.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc',keras_metrics.recall()])\n",
    "\n",
    "    model_lstm.summary()\n",
    "    return model_lstm\n",
    "\n",
    "def get_bi_lstm_model(mode, dropout, neurons, optimizer):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = Bidirectional(LSTM(neurons,input_shape=(MAX_SEQUENCE_LENGTH,), dropout=dropout),merge_mode= mode)(embedded_sequences)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model_lstm = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "    model_lstm.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc',keras_metrics.recall()])\n",
    "\n",
    "    model_lstm.summary()\n",
    "    return model_lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "def build_convnet():\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model_convnet = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "    model_convnet.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc',keras_metrics.recall()])\n",
    "    return model_convnet\n",
    "    \n",
    "    \n",
    "model_convnet = build_convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "### calculating class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1/y_val[:, 0].mean(), 1/y_val[:, 1].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model scores with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run(sub_dir=None):\n",
    "    \n",
    "    if sub_dir is not None:\n",
    "        log_dir = os.path.join(LOG_DIR, sub_dir)\n",
    "    else:\n",
    "        log_dir = LOG_DIR\n",
    "    \n",
    "    return TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "                       write_graph=True, write_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epoch_no):\n",
    "    \n",
    "    start=datetime.now()\n",
    "    hist = model.fit(x_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=epoch_no,\n",
    "            validation_data=(x_val, y_val),\n",
    "            shuffle=True,\n",
    "            class_weight=weights,\n",
    "            verbose = 0\n",
    "        )  \n",
    "\n",
    "    runtime=datetime.now() - start\n",
    "    \n",
    "    loss = hist.history['loss'][0]    \n",
    "    return (model,loss, runtime)\n",
    " \n",
    "\n",
    "def get_scores(model, threshhold):\n",
    "    prediction = model.predict(x_val)\n",
    "    y_classes = ( [0 if x < threshhold else 1 for x in prediction[:,1]])\n",
    "    y_value = y_val.argmax(axis=1)\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_value,y_classes).ravel()\n",
    "    return (tn, fp, fn, tp)\n",
    "\n",
    "\n",
    "def get_pred(model):\n",
    "    prediction = model.predict(x_val)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_scores_pred(pred, threshhold):\n",
    "    y_classes = ( [0 if x < threshhold else 1 for x in pred[:,1]])\n",
    "    y_value = y_val.argmax(axis=1)\n",
    "    (tn, fp, fn, tp) = sklearn.metrics.confusion_matrix(y_value,y_classes).ravel()\n",
    "    return (tn, fp, fn, tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Various LSTM models\n",
    "Comparing LSTM forward, LSTM backward and Bi-directional LSTM\n",
    "\n",
    "####Trained Dataset : 4000 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm forwards\n",
    "print('lstm forwards')\n",
    "model = get_lstm_model(False,dropout,neurons,optimizer)\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "forward_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.describe(forward_pred[:,1]))\n",
    "threshhold=0.006\n",
    "(tn, fp, fn, tp) = get_scores_pred(forward_pred, threshhold)\n",
    "forward_scores =(tn, fp, fn, tp)\n",
    "print(forward_scores)\n",
    "forward_result = ('forward',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm backwards\n",
    "print('lstm_back')\n",
    "model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "backward_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.describe(backward_pred[:,1]))\n",
    "threshhold=0.001\n",
    "(tn, fp, fn, tp) = get_scores_pred(backward_pred, threshhold)\n",
    "backward_scores = (tn, fp, fn, tp)\n",
    "print(tn, fp, fn, tp)\n",
    "backward_result = ('backward',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional concat\n",
    "print('bidirectional concat')\n",
    "model = get_bi_lstm_model('concat', 0,neurons,optimizer)\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "bilstm_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.describe(bilstm_pred[:,1]))\n",
    "threshhold=0.004\n",
    "(tn, fp, fn, tp) = get_scores_pred(bilstm_pred, threshhold)\n",
    "bi_scores = (tn, fp, fn, tp)\n",
    "print(tn, fp, fn, tp)\n",
    "bi_result = ('bidirectional',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())\n",
    "\n",
    "# # line plot of results\n",
    "# results.plot()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "As it can be seen in the following table, for 5 and 10 epochs, without any dropout, backward LSTM achieve the best result. For each model, we found the best threshhold to meet our evaluation criteria.\n",
    "For the rest of our analysis we select backward LSTM since it achieve the best results in a shorter amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PrettyTable(['Model','threshhold','TN', 'FP', 'FN','TP','Loss','RunTime (sec)'])\n",
    "\n",
    "t.add_row(forward_result)\n",
    "t.add_row(backward_result)\n",
    "t.add_row(bi_result)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we start to reduce the number of trained datasets\n",
    "####  Trained Dataset : 500 papers, included papers: 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model on 500 datapoints\n",
    "x_train, x_val, y_train, y_val = split_data(0.9, 16)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm backwards\n",
    "print('lstm_back')\n",
    "model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "(trained_model,loss500, runtime500) = train_model(model, epochs)\n",
    "backward_pred500 = get_pred(trained_model)\n",
    "print('loss:' ,loss500,'runtime:', runtime500.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.describe(backward_pred500[:,1]))\n",
    "threshhold=0.03\n",
    "(tn, fp, fn, tp) = get_scores_pred(backward_pred500, threshhold)\n",
    "backward_scores500 = (tn, fp, fn, tp)\n",
    "print(backward_scores500)\n",
    "backward_result500 = ('500 papers',threshhold,tn, fp, fn, tp, loss500, runtime500.total_seconds())\n",
    "print(backward_result500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Trained Dataset : 100 papers, included papers: 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model on 100 datapoints\n",
    "x_train, x_val, y_train, y_val = split_data(0.98, 20)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0.25\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm backwards\n",
    "print('lstm_back')\n",
    "model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "(trained_model,loss100, runtime100) = train_model(model, epochs)\n",
    "backward_pred100 = get_pred(trained_model)\n",
    "print('loss:' ,loss100,'runtime:', runtime100.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.describe(backward_pred100[:,1]))\n",
    "threshhold=0.15\n",
    "(tn, fp, fn, tp) = get_scores_pred(backward_pred100, threshhold)\n",
    "backward_scores100 = (tn, fp, fn, tp)\n",
    "print(backward_scores100)\n",
    "backward_result100 = ('100 papers',threshhold,tn, fp, fn, tp, loss100, runtime100.total_seconds())\n",
    "print(backward_result100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Trained Dataset : 50 papers, included papers: 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model on 100 datapoints\n",
    "x_train, x_val, y_train, y_val = split_data(0.99, 10)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0.3\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm backwards\n",
    "print('lstm_back')\n",
    "model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "(trained_model,loss50, runtime50) = train_model(model, epochs)\n",
    "backward_pred50 = get_pred(trained_model)\n",
    "print('loss:' ,loss50,'runtime:', runtime50.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.describe(backward_pred100[:,1]))\n",
    "threshhold=0.14\n",
    "(tn, fp, fn, tp) = get_scores_pred(backward_pred50, threshhold)\n",
    "backward_scores50 = (tn, fp, fn, tp)\n",
    "print(backward_scores50)\n",
    "backward_result50 = ('50 papers',threshhold,tn, fp, fn, tp, loss50, runtime50.total_seconds())\n",
    "print(backward_result50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_result4000 =('4000 papaers',*backward_result[1:]) \n",
    "col_labels = ['train_dataset','threshhold','TN', 'FP', 'FN','TP','Loss','RunTime (sec)']\n",
    "data_rows = [backward_result4000,backward_result500,backward_result100,backward_result50]\n",
    "df = pd.DataFrame.from_records(data_rows, columns=col_labels)\n",
    "\n",
    "df['FP_rate']=df.loc[:,'FP']/(df.loc[:,'FP']+df.loc[:,'TN'])\n",
    "df['Recall']=df.loc[:,'TP']/(df.loc[:,'TP']+df.loc[:,'FN'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = PrettyTable(['#train dataset','threshhold','TN', 'FP', 'FN','TP','Loss','RunTime (sec)'])\n",
    "\n",
    "tt.add_row(backward_result4000)\n",
    "tt.add_row(backward_result500)\n",
    "tt.add_row(backward_result100)\n",
    "tt.add_row(backward_result50)\n",
    "\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_fp_rate = pd.DataFrame([['FP_rate',4000,0.077],['FP_rate',500,0.854],['FP_rate',100,0.904],['FP_rate',50,0.846],\n",
    "                   ['Recall',4000,0.875],['Recall',500,0.950],['Recall',100,0.947],['Recall',50,0.966]],\n",
    "                    columns=['score','trained_dataset','val'])\n",
    "\n",
    "df_fp_rate.pivot(\"trained_dataset\", \"score\", \"val\").plot(kind='barh')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=2)\n",
    "plt.title('Model scores based on size of the training dataset')\n",
    "plt.show()\n",
    "\n",
    "# df_tn_fp = pd.DataFrame([['FP','4000',78],['FP','500',3876],['FP','100',4467],['FP','50',4223],\n",
    "#                    ['TN','4000',930],['TN','500',678],['TN','100',470],['TN','50',768]],columns=['score','trained_dataset','val'])\n",
    "\n",
    "\n",
    "# df_tn_fp.pivot(\"trained_dataset\", \"score\", \"val\").plot(kind='bar')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We added 15 positive cases to training, it causes that model label all data as positive. Now I try with stratified samples.\n",
    "\n",
    "### Convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training dataset size = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = split_data(0.2, 0)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0.3\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convnet\n",
    "print('convnet')\n",
    "model = build_convnet()\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "forward_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.describe(forward_pred[:,1]))\n",
    "threshhold=0.00000001\n",
    "(tn, fp, fn, tp) = get_scores_pred(forward_pred, threshhold)\n",
    "cnn_score = (tn, fp, fn, tp)\n",
    "print(cnn_score)\n",
    "cnn_result = ('4000 papers',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())\n",
    "print(cnn_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
