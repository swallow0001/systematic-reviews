{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Ideas\n",
    "\n",
    "- symantic web technology, micro applications (combine the approaches)\n",
    "- the role of authors, citations\n",
    "- sensitivity 1 doesn't work. (multiple raters, different expertise)\n",
    "- citation networks\n",
    "- use of rules how to interpreted the text. \n",
    "\n",
    "\n",
    "## Remarks\n",
    "\n",
    "- Covidence and Rayyan (software)\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "# Systematic review\n",
    "\n",
    "Based on: https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "\n",
    "> This script loads pre-trained word embeddings (GloVe embeddings)\n",
    "into a frozen Keras Embedding layer, and uses it to\n",
    "train a text classification model on the 20 Newsgroup dataset\n",
    "(classification of newsgroup messages into 20 different categories).\n",
    "GloVe embedding data can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "20 Newsgroup data can be found at:\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n",
    "\n",
    "\n",
    "## Related links\n",
    "https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\n",
    "https://www.quora.com/What-deep-learning-method-to-use-to-classify-text-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras_metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from prettytable import PrettyTable\n",
    "from numpy import cumsum\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Project properties\n",
    "\n",
    "Basic output properties related to the folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# OUTPUT_DIR = os.path.join(\"..\", \"output\")\n",
    "\n",
    "LOG_DIR = os.path.join(\"..\",\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "- minimize number of papers\n",
    "- max # of fn =1\n",
    "- threshhold >0\n",
    "\n",
    "## Data\n",
    "\n",
    "Basic data related variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the data folder\n",
    "BASE_DIR = os.path.join(\"..\", \"data\")\n",
    "\n",
    "# the folder with the word2vec \n",
    "GLOVE_DIR = os.path.join(\"..\", \"word2vec\")\n",
    "\n",
    "# the papers about PTSD\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, \"ptsd_review\", \"csv\")\n",
    "\n",
    "# the target variable\n",
    "TARGET_VARIABLE = \"included_final\" # \"included_ats\" (after title screening)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Read data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 5077 texts.\n"
     ]
    }
   ],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "data_path = os.path.join(TEXT_DATA_DIR, \"schoot-lgmm-ptsd-traindata.csv\")\n",
    "full_data = pd.read_csv(data_path)\n",
    "\n",
    "texts = (full_data['title'].fillna('') + ' ' + full_data['abstract'].fillna(''))\n",
    "labels = full_data[TARGET_VARIABLE]\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word tokenizing\n",
    "\n",
    "Apply vectorization to texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26396 unique tokens.\n",
      "Shape of data tensor: (5077, 1000)\n",
      "Shape of label tensor: (5077, 2)\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# get the word index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, \n",
    "                     maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding='post', truncating='post')\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create training and validation set\n",
    "Split the dataset into a training and a validation set. Use SciKit-learn for this. \n",
    "We decided to train our model on a small dataset (around 500), to make the environment more simulated to the active learning approach where we have small number of labeled data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4061, 1000) , x_val shape: (1016, 1000)\n",
      "y_train shape: (4061, 2) , y_val shape: (1016, 2)\n",
      "included papers in train dataset: 32\n",
      "included papers in test dataset: 8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(validation_split, added_positives):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        data,\n",
    "        labels,\n",
    "        test_size=validation_split,\n",
    "        random_state=2018,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    # add added_positives positive paper to training dataset\n",
    "    if added_positives>0 :\n",
    "        positive_indx = np.where(y_val[:,1]==1)[0]\n",
    "        x_train = np.vstack((x_train, x_val[positive_indx[0:added_positives]]))\n",
    "        y_train = np.vstack((y_train,y_val[positive_indx[0:added_positives]]))\n",
    "\n",
    "        x_val = np.delete(x_val, positive_indx[0:added_positives],0)\n",
    "        y_val = np.delete(y_val, positive_indx[0:added_positives],0)\n",
    "\n",
    "    return (x_train, x_val, y_train, y_val)\n",
    "\n",
    "x_train, x_val, y_train, y_val = split_data(0.2, 0)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## set dimensions\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 2518927 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'wiki.en.vec'), encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        values = line.split()\n",
    "        split_on_i = len(values) - EMBEDDING_DIM\n",
    "        word = ' '.join(values[0:split_on_i])\n",
    "        coefs = np.asarray(values[split_on_i:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix:  (20000, 300)\n"
     ]
    }
   ],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        \n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "                \n",
    "print('Shape of embedding matrix: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Make an Keras embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "def build_embedding(weights):\n",
    "    return Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[weights],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "embedding_layer = build_embedding(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Models\n",
    "\n",
    "This section contains models to use later on. Focusing on Convolutional and LSTM models. \n",
    "\n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_lstm_model(backwards, dropout, neurons,optimizer ):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = LSTM(neurons,input_shape=(MAX_SEQUENCE_LENGTH,),  go_backwards=backwards, dropout=dropout)(embedded_sequences)\n",
    "    x = Dense(128,activation='relu')(x)\n",
    "    output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model_lstm = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "    model_lstm.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc',keras_metrics.recall()])\n",
    "\n",
    "    model_lstm.summary()\n",
    "    return model_lstm\n",
    "\n",
    "def get_bi_lstm_model(mode, dropout, neurons, optimizer):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = Bidirectional(LSTM(neurons,input_shape=(MAX_SEQUENCE_LENGTH,), dropout=dropout),merge_mode= mode)(embedded_sequences)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model_lstm = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "    model_lstm.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc',keras_metrics.recall()])\n",
    "\n",
    "    model_lstm.summary()\n",
    "    return model_lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "def build_convnet():\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model_convnet = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "    model_convnet.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc',keras_metrics.recall()])\n",
    "    return model_convnet\n",
    "    \n",
    "    \n",
    "model_convnet = build_convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the model\n",
    "\n",
    "### calculating class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weights = [1/y_val[:, 0].mean(), 1/y_val[:, 1].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualize model scores with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_run(sub_dir=None):\n",
    "    \n",
    "    if sub_dir is not None:\n",
    "        log_dir = os.path.join(LOG_DIR, sub_dir)\n",
    "    else:\n",
    "        log_dir = LOG_DIR\n",
    "    \n",
    "    return TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "                       write_graph=True, write_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, epoch_no):\n",
    "    \n",
    "    start=datetime.now()\n",
    "    hist = model.fit(x_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=epoch_no,\n",
    "            validation_data=(x_val, y_val),\n",
    "            shuffle=True,\n",
    "            class_weight=weights,\n",
    "            verbose = 0\n",
    "        )  \n",
    "\n",
    "    runtime=datetime.now() - start\n",
    "    \n",
    "    loss = hist.history['loss'][0]    \n",
    "    return (model,loss, runtime)\n",
    " \n",
    "\n",
    "def get_scores(model, threshhold):\n",
    "    prediction = model.predict(x_val)\n",
    "    y_classes = ( [0 if x < threshhold else 1 for x in prediction[:,1]])\n",
    "    y_value = y_val.argmax(axis=1)\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_value,y_classes).ravel()\n",
    "    return (tn, fp, fn, tp)\n",
    "\n",
    "\n",
    "def get_pred(model):\n",
    "    prediction = model.predict(x_val)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_scores_pred(pred, threshhold):\n",
    "    y_classes = ( [0 if x < threshhold else 1 for x in pred[:,1]])\n",
    "    y_value = y_val.argmax(axis=1)\n",
    "    (tn, fp, fn, tp) = sklearn.metrics.confusion_matrix(y_value,y_classes).ravel()\n",
    "    return (tn, fp, fn, tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compare Various LSTM models\n",
    "Comparing LSTM forward, LSTM backward and Bi-directional LSTM\n",
    "\n",
    "####Trained Dataset : 4000 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm forwards\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,014,106\n",
      "Trainable params: 14,106\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-2926fd5740be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lstm forwards'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lstm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mneurons\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mforward_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss:'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'runtime:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-147ce3c7dfc1>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, epoch_no)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         )  \n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lstm forwards\n",
    "print('lstm forwards')\n",
    "model = get_lstm_model(False,dropout,neurons,optimizer)\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "forward_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(stats.describe(forward_pred[:,1]))\n",
    "threshhold=0.006\n",
    "(tn, fp, fn, tp) = get_scores_pred(forward_pred, threshhold)\n",
    "forward_scores =(tn, fp, fn, tp)\n",
    "print(forward_scores)\n",
    "forward_result = ('forward',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_back\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,014,106\n",
      "Trainable params: 14,106\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "loss: 0.17255625873 runtime: 846.967546\n"
     ]
    }
   ],
   "source": [
    "# lstm backwards\n",
    "print('lstm_back')\n",
    "model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "backward_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=1016, minmax=(5.8844416e-06, 0.44997516), mean=0.0043556751, variance=0.00098875398, skewness=9.736503601074219, kurtosis=104.50798212983028)\n",
      "930 78 1 7\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(backward_pred[:,1]))\n",
    "threshhold=0.001\n",
    "(tn, fp, fn, tp) = get_scores_pred(backward_pred, threshhold)\n",
    "backward_scores = (tn, fp, fn, tp)\n",
    "print(tn, fp, fn, tp)\n",
    "backward_result = ('backward',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirectional concat\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 20)                24880     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               2688      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,027,826\n",
      "Trainable params: 27,826\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "loss: 0.160602745378 runtime: 1016.731862\n"
     ]
    }
   ],
   "source": [
    "# bidirectional concat\n",
    "print('bidirectional concat')\n",
    "model = get_bi_lstm_model('concat', 0,neurons,optimizer)\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "bilstm_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=1016, minmax=(7.5686476e-06, 0.47874796), mean=0.0029803477, variance=0.00079876237, skewness=13.227283477783203, kurtosis=184.58149405647666)\n",
      "978 30 1 7\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(bilstm_pred[:,1]))\n",
    "threshhold=0.004\n",
    "(tn, fp, fn, tp) = get_scores_pred(bilstm_pred, threshhold)\n",
    "bi_scores = (tn, fp, fn, tp)\n",
    "print(tn, fp, fn, tp)\n",
    "bi_result = ('bidirectional',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())\n",
    "\n",
    "# # line plot of results\n",
    "# results.plot()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('backward', 0.005, 0, 4534, 0, 20, 0.4690955298352652, 620.512003)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Result\n",
    "As it can be seen in the following table, for 5 and 10 epochs, without any dropout, backward LSTM achieve the best result. For each model, we found the best threshhold to meet our evaluation criteria.\n",
    "For the rest of our analysis we select backward LSTM since it achieve the best results in a shorter amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----+------+----+----+----------------+---------------+\n",
      "|     Model     | threshhold |  TN |  FP  | FN | TP |      Loss      | RunTime (sec) |\n",
      "+---------------+------------+-----+------+----+----+----------------+---------------+\n",
      "|    forward    |   0.006    |  1  | 1007 | 0  | 8  | 0.524007058238 |   271.696444  |\n",
      "|    backward   |   0.005    | 983 |  25  | 1  | 7  | 0.198787400083 |   734.547185  |\n",
      "| bidirectional |   0.004    | 978 |  30  | 1  | 7  | 0.160602745378 |  1016.731862  |\n",
      "+---------------+------------+-----+------+----+----+----------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "t = PrettyTable(['Model','threshhold','TN', 'FP', 'FN','TP','Loss','RunTime (sec)'])\n",
    "\n",
    "t.add_row(forward_result)\n",
    "t.add_row(backward_result)\n",
    "t.add_row(bi_result)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we start to reduce the number of trained datasets\n",
    "####  Trained Dataset : 500 papers, included papers: 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (523, 1000) , x_val shape: (4554, 1000)\n",
      "y_train shape: (523, 2) , y_val shape: (4554, 2)\n",
      "included papers in train dataset: 20\n",
      "included papers in test dataset: 20\n"
     ]
    }
   ],
   "source": [
    "## train model on 500 datapoints\n",
    "x_train, x_val, y_train, y_val = split_data(0.9, 16)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_back\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,014,106\n",
      "Trainable params: 14,106\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "loss: 0.469095529835 runtime: 620.512003\n"
     ]
    }
   ],
   "source": [
    "# lstm backwards\n",
    "print('lstm_back')\n",
    "model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "(trained_model,loss500, runtime500) = train_model(model, epochs)\n",
    "backward_pred500 = get_pred(trained_model)\n",
    "print('loss:' ,loss500,'runtime:', runtime500.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=4554, minmax=(0.020136083, 0.11657762), mean=0.044118702, variance=0.00015918662, skewness=0.8438590168952942, kurtosis=1.5377718690381652)\n",
      "(658, 3876, 1, 19)\n",
      "('500 papers', 0.03, 658, 3876, 1, 19, 0.4690955298352652, 620.512003)\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(backward_pred500[:,1]))\n",
    "threshhold=0.03\n",
    "(tn, fp, fn, tp) = get_scores_pred(backward_pred500, threshhold)\n",
    "backward_scores500 = (tn, fp, fn, tp)\n",
    "print(backward_scores500)\n",
    "backward_result500 = ('500 papers',threshhold,tn, fp, fn, tp, loss500, runtime500.total_seconds())\n",
    "print(backward_result500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Trained Dataset : 100 papers, included papers: 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (121, 1000) , x_val shape: (4956, 1000)\n",
      "y_train shape: (121, 2) , y_val shape: (4956, 2)\n",
      "included papers in train dataset: 21\n",
      "included papers in test dataset: 19\n"
     ]
    }
   ],
   "source": [
    "## train model on 100 datapoints\n",
    "x_train, x_val, y_train, y_val = split_data(0.98, 20)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0.25\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_back\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,014,106\n",
      "Trainable params: 14,106\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "loss: 0.730353176594 runtime: 839.615404\n"
     ]
    }
   ],
   "source": [
    "# lstm backwards\n",
    "print('lstm_back')\n",
    "model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "(trained_model,loss100, runtime100) = train_model(model, epochs)\n",
    "backward_pred100 = get_pred(trained_model)\n",
    "print('loss:' ,loss100,'runtime:', runtime100.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=4956, minmax=(0.11212229, 0.36471102), mean=0.20409906, variance=0.0016735594, skewness=0.37832343578338623, kurtosis=-0.07060792000945115)\n",
      "(470, 4467, 1, 18)\n",
      "('100 papers', 0.15, 470, 4467, 1, 18, 0.73035317659378052, 839.615404)\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(backward_pred100[:,1]))\n",
    "threshhold=0.15\n",
    "(tn, fp, fn, tp) = get_scores_pred(backward_pred100, threshhold)\n",
    "backward_scores100 = (tn, fp, fn, tp)\n",
    "print(backward_scores100)\n",
    "backward_result100 = ('100 papers',threshhold,tn, fp, fn, tp, loss100, runtime100.total_seconds())\n",
    "print(backward_result100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Trained Dataset : 50 papers, included papers: 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60, 1000) , x_val shape: (5017, 1000)\n",
      "y_train shape: (60, 2) , y_val shape: (5017, 2)\n",
      "included papers in train dataset: 10\n",
      "included papers in test dataset: 30\n"
     ]
    }
   ],
   "source": [
    "## train model on 100 datapoints\n",
    "x_train, x_val, y_train, y_val = split_data(0.99, 10)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0.3\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_back\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,014,106\n",
      "Trainable params: 14,106\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "loss: 0.69319164753 runtime: 760.558599\n"
     ]
    }
   ],
   "source": [
    "# lstm backwards\n",
    "print('lstm_back')\n",
    "model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "(trained_model,loss50, runtime50) = train_model(model, epochs)\n",
    "backward_pred50 = get_pred(trained_model)\n",
    "print('loss:' ,loss50,'runtime:', runtime50.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=4956, minmax=(0.11212229, 0.36471102), mean=0.20409906, variance=0.0016735594, skewness=0.37832343578338623, kurtosis=-0.07060792000945115)\n",
      "(764, 4223, 1, 29)\n",
      "('50 papers', 0.14, 764, 4223, 1, 29, 0.69319164752960205, 760.558599)\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(backward_pred100[:,1]))\n",
    "threshhold=0.14\n",
    "(tn, fp, fn, tp) = get_scores_pred(backward_pred50, threshhold)\n",
    "backward_scores50 = (tn, fp, fn, tp)\n",
    "print(backward_scores50)\n",
    "backward_result50 = ('50 papers',threshhold,tn, fp, fn, tp, loss50, runtime50.total_seconds())\n",
    "print(backward_result50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train_dataset  threshhold   TN    FP  FN  TP      Loss  RunTime (sec)  \\\n",
      "0  4000 papaers       0.001  930    78   1   7  0.172556     846.967546   \n",
      "1    500 papers       0.030  658  3876   1  19  0.469096     620.512003   \n",
      "2    100 papers       0.150  470  4467   1  18  0.730353     839.615404   \n",
      "3     50 papers       0.140  764  4223   1  29  0.693192     760.558599   \n",
      "\n",
      "    FP_rate    Recall  \n",
      "0  0.077381  0.875000  \n",
      "1  0.854874  0.950000  \n",
      "2  0.904800  0.947368  \n",
      "3  0.846802  0.966667  \n"
     ]
    }
   ],
   "source": [
    "backward_result4000 =('4000 papaers',*backward_result[1:]) \n",
    "col_labels = ['train_dataset','threshhold','TN', 'FP', 'FN','TP','Loss','RunTime (sec)']\n",
    "data_rows = [backward_result4000,backward_result500,backward_result100,backward_result50]\n",
    "df = pd.DataFrame.from_records(data_rows, columns=col_labels)\n",
    "\n",
    "df['FP_rate']=df.loc[:,'FP']/(df.loc[:,'FP']+df.loc[:,'TN'])\n",
    "df['Recall']=df.loc[:,'TP']/(df.loc[:,'TP']+df.loc[:,'FN'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-----+------+----+----+----------------+---------------+\n",
      "| #train dataset | threshhold |  TN |  FP  | FN | TP |      Loss      | RunTime (sec) |\n",
      "+----------------+------------+-----+------+----+----+----------------+---------------+\n",
      "|  4000 papaers  |   0.001    | 930 |  78  | 1  | 7  | 0.17255625873  |   846.967546  |\n",
      "|   500 papers   |    0.03    | 658 | 3876 | 1  | 19 | 0.469095529835 |   620.512003  |\n",
      "|   100 papers   |    0.15    | 470 | 4467 | 1  | 18 | 0.730353176594 |   839.615404  |\n",
      "|   50 papers    |    0.14    | 764 | 4223 | 1  | 29 | 0.69319164753  |   760.558599  |\n",
      "+----------------+------------+-----+------+----+----+----------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "tt = PrettyTable(['#train dataset','threshhold','TN', 'FP', 'FN','TP','Loss','RunTime (sec)'])\n",
    "\n",
    "tt.add_row(backward_result4000)\n",
    "tt.add_row(backward_result500)\n",
    "tt.add_row(backward_result100)\n",
    "tt.add_row(backward_result50)\n",
    "\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEaCAYAAAA/lAFyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24f50879438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_fp_rate = pd.DataFrame([['FP_rate',4000,0.077],['FP_rate',500,0.854],['FP_rate',100,0.904],['FP_rate',50,0.846],\n",
    "                   ['Recall',4000,0.875],['Recall',500,0.950],['Recall',100,0.947],['Recall',50,0.966]],\n",
    "                    columns=['score','trained_dataset','val'])\n",
    "\n",
    "df_fp_rate.pivot(\"trained_dataset\", \"score\", \"val\").plot(kind='barh')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=2)\n",
    "plt.title('Model scores based on size of the training dataset')\n",
    "plt.show()\n",
    "\n",
    "# df_tn_fp = pd.DataFrame([['FP','4000',78],['FP','500',3876],['FP','100',4467],['FP','50',4223],\n",
    "#                    ['TN','4000',930],['TN','500',678],['TN','100',470],['TN','50',768]],columns=['score','trained_dataset','val'])\n",
    "\n",
    "\n",
    "# df_tn_fp.pivot(\"trained_dataset\", \"score\", \"val\").plot(kind='bar')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We added 15 positive cases to training, it causes that model label all data as positive. Now I try with stratified samples.\n",
    "\n",
    "### Convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training dataset size = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4061, 1000) , x_val shape: (1016, 1000)\n",
      "y_train shape: (4061, 2) , y_val shape: (1016, 2)\n",
      "included papers in train dataset: 32\n",
      "included papers in test dataset: 8\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = split_data(0.2, 0)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print(\"included papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "print(\"included papers in test dataset:\",(y_val[:,1]==1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "BATCH_SIZE = 128\n",
    "dropout=0.3\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnet\n",
      "loss: 0.0712402132024 runtime: 1279.113783\n"
     ]
    }
   ],
   "source": [
    "# convnet\n",
    "print('convnet')\n",
    "model = build_convnet()\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "forward_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=1016, minmax=(1.4847232e-17, 0.0066562979), mean=1.7063758e-05, variance=7.9844718e-08, skewness=20.184995651245117, kurtosis=423.9775680582843)\n",
      "(676, 332, 2, 6)\n",
      "('4000 papers', 1e-08, 676, 332, 2, 6, 0.071240213202358224, 1279.113783)\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(forward_pred[:,1]))\n",
    "threshhold=0.00000001\n",
    "(tn, fp, fn, tp) = get_scores_pred(forward_pred, threshhold)\n",
    "cnn_score = (tn, fp, fn, tp)\n",
    "print(cnn_score)\n",
    "cnn_result = ('4000 papers',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())\n",
    "print(cnn_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
