{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Notes \n",
    "\n",
    "- rens van der schoot\n",
    "- \n",
    "\n",
    "\n",
    "## Ideas\n",
    "\n",
    "- symantic web technology, micro applications (combine the approaches)\n",
    "- the role of authors, citations\n",
    "- sensitivity 1 doesn't work. (multiple raters, different expertise)\n",
    "- citation networks\n",
    "- use of rules how to interpreted the text. \n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Remarks\n",
    "\n",
    "- Covidence and Rayyan (software)\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "# Systematic review\n",
    "\n",
    "Based on: https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "\n",
    "> This script loads pre-trained word embeddings (GloVe embeddings)\n",
    "into a frozen Keras Embedding layer, and uses it to\n",
    "train a text classification model on the 20 Newsgroup dataset\n",
    "(classification of newsgroup messages into 20 different categories).\n",
    "GloVe embedding data can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "20 Newsgroup data can be found at:\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n",
    "\n",
    "\n",
    "## Related links\n",
    "https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\n",
    "https://www.quora.com/What-deep-learning-method-to-use-to-classify-text-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras_metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from prettytable import PrettyTable\n",
    "from numpy import cumsum\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Project properties\n",
    "\n",
    "Basic output properties related to the folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# OUTPUT_DIR = os.path.join(\"..\", \"output\")\n",
    "\n",
    "LOG_DIR = os.path.join(\"..\",\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "- minimize number of papers\n",
    "- max # of fn =1\n",
    "- threshhold >0\n",
    "\n",
    "## Data\n",
    "\n",
    "Basic data related variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the data folder\n",
    "BASE_DIR = os.path.join(\"..\", \"data\")\n",
    "\n",
    "# the folder with the word2vec \n",
    "GLOVE_DIR = os.path.join(\"..\", \"word2vec\")\n",
    "\n",
    "# the papers about PTSD\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, \"ptsd_review\", \"csv\")\n",
    "\n",
    "# the target variable\n",
    "TARGET_VARIABLE = \"included_final\" # \"included_ats\" (after title screening)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Read data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 5077 texts.\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "data_path = os.path.join(TEXT_DATA_DIR, \"schoot-lgmm-ptsd-traindata.csv\")\n",
    "full_data = pd.read_csv(data_path)\n",
    "\n",
    "texts = (full_data['title'].fillna('') + ' ' + full_data['abstract'].fillna(''))\n",
    "labels = full_data[TARGET_VARIABLE]\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=[0, 0, 0, 0, 17398, 18790, 19098, 1568, 8762, 10384, 8524, 15750, 22423, 8762, 10384, 10731, 11841, 16419, 7688, 6968, 22154, 11841, 1568, 3104, 18790, 18813, 2759, 15616, 16419, 1942, 5526, 16419, 7884, 15334, 13962, 12390, 16715, 3980, 18790, 19098, 1568, 824, 15026, 11841, 1568, 14202, 1411, 290, 23501, 6968, 15319, 20205, 18813, 13296, 17707, 16390, 14608, 18813, 3458, 2603, 18813, 19567, 6058, 18813, 22041, 18813, 6968, 24069, 15616, 24740, 18813, 11768, 16200, 16419, 824, 22733, 11841, 24943, 1568, 290, 3983, 6968, 24521, 18813, 15943, 18813, 5494, 18790, 18813, 6968, 3977, 5602, 16419, 17407, 12342, 11841, 24943, 1568, 290, 23501, 6833, 18813, 8188, 18813, 9341, 18813, 13229, 18813, 12819, 16878, 17563, 18523, 18813, 6968, 6899, 12390, 16419, 1905, 7372, 11841, 16419, 13612, 9829, 14202, 11161, 23962, 12390, 14416, 9881, 18813, 18343, 3974, 4472, 4182, 19525, 14222, 4834, 11701, 24355, 11669, 16419, 3458, 9896, 11841, 20205, 6968, 12213, 13316, 15341, 10351, 11841, 16419, 9168, 11841, 21219, 12390, 24796, 20510, 7668, 17655, 25333, 7289, 6968, 22653, 13316, 15341, 14241, 11841, 8524, 12390, 5720, 9752, 11841, 15950, 3245, 21836, 13322, 17399, 12390, 9314, 9150, 6968, 16419, 487, 24854, 9314, 21630, 16419, 7983, 21219, 11841, 1568, 3104, 18790, 8762, 10384, 3020, 4977, 7394, 1568, 3104, 18790, 16419, 5644, 947, 13537, 4025, 2510, 11841, 16419, 5894, 11841, 16419, 21219, 18813, 16419, 20361, 19013, 13962, 12390, 23402, 11768, 18813, 6968, 21836, 7884, 23489, 2366, 5013, 16419, 17587, 13962, 12390, 24612, 18518, 15682, 14202, 1047, 23962, 14841, 17776, 20894, 12185, 17655, 9648, 290, 16419, 19026, 12916, 1385, 4977, 17284, 16419, 12536, 11841, 3126, 23543, 17655, 12530, 14973, 16419, 13322, 24854, 1662, 6968, 23829, 18790, 290, 16419, 9752, 11841, 10920, 15950, 24943, 5232, 22451, 447, 6968, 20648, 13240, 14648, 14841, 3245, 16419, 6249, 17655, 13962, 15616, 16419, 6903, 11841, 18790, 17655, 23962, 290, 24898, 12390, 16715, 3980, 18790, 19567, 9314, 20279, 23006, 23178, 19536, 17998, 13965, 14202, 1047, 1411, 12530, 14973, 22223, 17864, 18629, 2851, 18813, 6495, 18813, 17276, 18813, 2442, 18813, 3417, 6968, 1905, 22166, 13156, 17361, 24854, 17398, 18790, 19098, 1568, 6968, 20499, 8390, 4902, 14202, 1047, 8693, 16419, 5644, 7080, 13537, 3020, 4977, 12812, 13322, 8253, 14202, 19610, 22028, 19098, 4025, 13669, 18551, 11841, 51, 5232, 24943, 18813, 16419, 5644, 1721, 12390, 12019, 6968, 4941, 10239, 15616, 23325, 18813, 3293, 18813, 265, 18813, 2442, 18813, 1847, 18813, 6968, 1905, 14843, 18813, 5232, 15796, 5232, 14184, 18813, 1905, 22563, 18813, 25140, 19552, 18813, 22210, 18813, 6968, 4807, 15718, 10730, 14973, 14843, 21719, 11608, 21637, 22235, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "np.array(t,dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word tokenizing\n",
    "\n",
    "Apply vectorization to texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26396 unique tokens.\n",
      "Shape of data tensor: (5077, 1000)\n",
      "Shape of label tensor: (5077, 2)\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# get the word index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, \n",
    "                     maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding='post', truncating='post')\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create training and validation set\n",
    "Split the dataset into a training and a validation set. Use SciKit-learn for this. \n",
    "We decided to train our model on a small dataset (around 500), to make the environment more simulated to the active learning approach where we have small number of labeled data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4061, 1000) , x_val shape: (1016, 1000)\n",
      "y_train shape: (4061, 2) , y_val shape: (1016, 2)\n",
      "32\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(validation_split, added_positives):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        data,\n",
    "        labels,\n",
    "        test_size=validation_split,\n",
    "        random_state=2018,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    # add added_positives positive paper to training dataset\n",
    "    if added_positives>0 :\n",
    "        positive_indx = np.where(y_val[:,1]==1)[0]\n",
    "        x_train = np.vstack((x_train, x_val[positive_indx[0:added_positives]]))\n",
    "        y_train = np.vstack((y_train,y_val[positive_indx[0:added_positives]]))\n",
    "\n",
    "        x_val = np.delete(x_val, positive_indx[0:added_positives],0)\n",
    "        y_val = np.delete(y_val, positive_indx[0:added_positives],0)\n",
    "\n",
    "    return (x_train, x_val, y_train, y_val)\n",
    "\n",
    "x_train, x_val, y_train, y_val = split_data(0.2, 0)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "print((y_train[:,1]==1).sum())\n",
    "print((y_val[:,1]==1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We added 15 positive cases to training, it causes that model label all data as positive. Now I try with stratified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## set dimensions\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 2518927 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'wiki.en.vec'), encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        values = line.split()\n",
    "        split_on_i = len(values) - EMBEDDING_DIM\n",
    "        word = ' '.join(values[0:split_on_i])\n",
    "        coefs = np.asarray(values[split_on_i:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix:  (20000, 300)\n"
     ]
    }
   ],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        \n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "                \n",
    "print('Shape of embedding matrix: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Make an Keras embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "def build_embedding(weights):\n",
    "    return Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[weights],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "embedding_layer = build_embedding(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Models\n",
    "\n",
    "This section contains models to use later on. Focusing on Convolutional and LSTM models. \n",
    "\n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_lstm_model(backwards, dropout, neurons,optimizer ):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = LSTM(neurons,input_shape=(MAX_SEQUENCE_LENGTH,),  go_backwards=backwards, dropout=dropout)(embedded_sequences)\n",
    "    x = Dense(128,activation='relu')(x)\n",
    "    output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model_lstm = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "    model_lstm.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc',keras_metrics.recall()])\n",
    "\n",
    "    model_lstm.summary()\n",
    "    return model_lstm\n",
    "\n",
    "def get_bi_lstm_model(mode, dropout, neurons, optimizer):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    x = Bidirectional(LSTM(neurons,input_shape=(MAX_SEQUENCE_LENGTH,), dropout=dropout),merge_mode= mode)(embedded_sequences)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model_lstm = Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "    model_lstm.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc',keras_metrics.recall()])\n",
    "\n",
    "    model_lstm.summary()\n",
    "    return model_lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # train a 1D convnet with global maxpooling\n",
    "# def build_convnet(embedded_sequences):\n",
    "#     x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "#     x = MaxPooling1D(5)(x)\n",
    "#     x = Conv1D(128, 5, activation='relu')(x)\n",
    "#     x = MaxPooling1D(5)(x)\n",
    "#     x = Conv1D(128, 5, activation='relu')(x)\n",
    "#     x = GlobalMaxPooling1D()(x)\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "#     model_convnet = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "#     model_convnet.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='rmsprop',\n",
    "#                   metrics=['acc',keras_metrics.recall()])\n",
    "#     return model_convnet\n",
    "    \n",
    "    \n",
    "# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# ##build convnet model\n",
    "# embedded_sequences = embedding_layer(sequence_input)\n",
    "# model_convnet = build_convnet(embedded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the model\n",
    "\n",
    "### calculating class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0079365041220509, 127.00000047311187]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [1/y_val[:, 0].mean(), 1/y_val[:, 1].mean()]\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualize model scores with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_run(sub_dir=None):\n",
    "    \n",
    "    if sub_dir is not None:\n",
    "        log_dir = os.path.join(LOG_DIR, sub_dir)\n",
    "    else:\n",
    "        log_dir = LOG_DIR\n",
    "    \n",
    "    return TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "                       write_graph=True, write_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, epoch_no):\n",
    "    \n",
    "    # fit model for one epoch on this sequence\n",
    "    start=datetime.now()\n",
    "    hist = model.fit(x_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=epoch_no,\n",
    "            validation_data=(x_val, y_val),\n",
    "            shuffle=True,\n",
    "            class_weight=weights,\n",
    "            verbose = 0\n",
    "        )  \n",
    "\n",
    "    runtime=datetime.now() - start\n",
    "    \n",
    "    loss = hist.history['loss'][0]    \n",
    "    return (model,loss, runtime)\n",
    " \n",
    "\n",
    "def get_scores(model, threshhold):\n",
    "    prediction = model.predict(x_val)\n",
    "    y_classes = ( [0 if x < threshhold else 1 for x in prediction[:,1]])\n",
    "    y_value = y_val.argmax(axis=1)\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_value,y_classes).ravel()\n",
    "    return (tn, fp, fn, tp)\n",
    "\n",
    "\n",
    "def get_pred(model):\n",
    "    prediction = model.predict(x_val)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_scores_pred(pred, threshhold):\n",
    "    y_classes = ( [0 if x < threshhold else 1 for x in pred[:,1]])\n",
    "    y_value = y_val.argmax(axis=1)\n",
    "    (tn, fp, fn, tp) = sklearn.metrics.confusion_matrix(y_value,y_classes).ravel()\n",
    "    return (tn, fp, fn, tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compare Various LSTM models\n",
    "Comparing LSTM forward, LSTM backward and Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "BATCH_SIZE = 128\n",
    "dropout=0\n",
    "neurons=10\n",
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm forwards\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,014,106\n",
      "Trainable params: 14,106\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "loss: 0.452999496285 runtime: 245.833196\n"
     ]
    }
   ],
   "source": [
    "# lstm forwards\n",
    "print('lstm forwards')\n",
    "model = get_lstm_model(False,dropout,neurons,optimizer)\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "forward_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=1016, minmax=(0.0004685364, 0.0081466679), mean=0.0081391074, variance=5.8025279e-08, skewness=-31.827655792236328, kurtosis=1011.0000683884579)\n",
      "1 1007 0 8\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(forward_pred[:,1]))\n",
    "threshhold=0.007\n",
    "(tn, fp, fn, tp) = get_scores_pred(forward_pred, threshhold)\n",
    "print(tn, fp, fn, tp)\n",
    "forward_result = ('forward',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_back\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,014,106\n",
      "Trainable params: 14,106\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "loss: 0.19080518443 runtime: 423.034718\n"
     ]
    }
   ],
   "source": [
    "# lstm backwards\n",
    "print('lstm_back')\n",
    "model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "backward_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=1016, minmax=(0.0010104601, 0.11843406), mean=0.0077425269, variance=8.161911e-05, skewness=6.076805591583252, kurtosis=55.52590563550463)\n",
      "708 300 2 6\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(backward_pred[:,1]))\n",
    "threshhold=0.008\n",
    "(tn, fp, fn, tp) = get_scores_pred(backward_pred, threshhold)\n",
    "print(tn, fp, fn, tp)\n",
    "backward_result = ('backward',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirectional concat\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 20)                24880     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               2688      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,027,826\n",
      "Trainable params: 27,826\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "loss: 0.219225970372 runtime: 6463.14421\n"
     ]
    }
   ],
   "source": [
    "# bidirectional concat\n",
    "print('bidirectional concat')\n",
    "model = get_bi_lstm_model('concat', 0,neurons,optimizer)\n",
    "(trained_model,loss, runtime) = train_model(model, epochs)\n",
    "bilstm_pred = get_pred(trained_model)\n",
    "print('loss:' ,loss,'runtime:', runtime.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=1016, minmax=(0.0010974816, 0.086877428), mean=0.012040269, variance=6.1617146e-05, skewness=3.0470035076141357, kurtosis=17.544552364179676)\n",
      "326 682 2 6\n",
      "+---------------+------------+-----+------+----+----+----------------+---------------+\n",
      "|     Model     | threshhold |  TN |  FP  | FN | TP |      Loss      | RunTime (sec) |\n",
      "+---------------+------------+-----+------+----+----+----------------+---------------+\n",
      "|    forward    |   0.007    |  1  | 1007 | 0  | 8  |     0.452      |    245.833    |\n",
      "|    backward   |   0.008    | 708 | 300  | 2  | 6  |      0.19      |    423.034    |\n",
      "| bidirectional |   0.008    | 326 | 682  | 2  | 6  | 0.219225970372 |   6463.14421  |\n",
      "+---------------+------------+-----+------+----+----+----------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(bilstm_pred[:,1]))\n",
    "threshhold=0.008\n",
    "(tn, fp, fn, tp) = get_scores_pred(bilstm_pred, threshhold)\n",
    "print(tn, fp, fn, tp)\n",
    "bi_result = ('bidirectional',threshhold,tn, fp, fn, tp, loss, runtime.total_seconds())\n",
    "\n",
    "# # line plot of results\n",
    "# results.plot()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Result\n",
    "As it can be seen in the following table, for 5 epochs, without any dropout, backward LSTM achieve the best result. For each model, we found the bestt threshhold to meet our evaluation criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t = PrettyTable(['Model','threshhold','TN', 'FP', 'FN','TP','Loss','RunTime (sec)'])\n",
    "\n",
    "t.add_row(forward_result)\n",
    "t.add_row(backward_result)\n",
    "t.add_row(bi_result)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# results = DataFrame()\n",
    "# # sum merge\n",
    "# model = get_bi_lstm_model(embedded_sequences, 'sum')\n",
    "# results['bilstm_sum'] = train_model(model, 4)\n",
    "# # mul merge\n",
    "# model = get_bi_lstm_model(embedded_sequences, 'mul')\n",
    "# results['bilstm_mul'] = train_model(model, 4)\n",
    "# # avg merge\n",
    "# model = get_bi_lstm_model(embedded_sequences, 'ave')\n",
    "# results['bilstm_ave'] = train_model(model, 4)\n",
    "# # concat merge\n",
    "# model = get_bi_lstm_model(embedded_sequences, 'concat')\n",
    "# results['bilstm_con'] = train_model(model, 4)\n",
    "# # line plot of results\n",
    "# results.plot()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Grid Search on hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "def get_threshholds(pred):\n",
    "    desc= stats.describe(pred[:,1])\n",
    "    (min_prob,max_prob) = desc.minmax\n",
    "    step = (max_prob - min_prob)/10\n",
    "    threshholds = ','.join(str(i) for i in range(min_prob,max_prob+step,step))\n",
    "    return threshholds\n",
    "\n",
    "def run_model(model_name,neurons,optimizer,epoch, dropout):\n",
    "    \n",
    "    all_scores = list()\n",
    "    model = None\n",
    "    if model_name =='bidirectional':\n",
    "        print('lstm_bidirectional')\n",
    "        model = get_bi_lstm_model('concat', dropout,neurons,optimizer)\n",
    "    else:\n",
    "        if model_name =='backward':\n",
    "            print('lstm_backward')\n",
    "            model = get_lstm_model(True,dropout,neurons,optimizer)\n",
    "\n",
    "        \n",
    "    (trained_model,loss, runtime) = train_model(model, epoch)\n",
    "    pred = get_pred(trained_model)\n",
    "    \n",
    "    ## generate a sequence of 10 threshhold probabilities\n",
    "    threshholds = get_threshholds(pred)\n",
    "    \n",
    "    ## calculate scores for each threshhold\n",
    "    for threshhold in threshholds:\n",
    "        scores = get_scores(trained_model, threshhold)\n",
    "        scores_row = (model_name, neurons,optimizer,epoch, dropout,threshhold,scores,loss, runtime.total_seconds())\n",
    "        all.scores.append(scores_row)\n",
    "        \n",
    "    return all_scores\n",
    "      \n",
    "     \n",
    "        \n",
    "        \n",
    "def grid_search_lstm(validation_set, added_positives):        \n",
    "    \n",
    "    x_train, x_val, y_train, y_val = split_data(validation_set, added_positives)\n",
    "\n",
    "    print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "    print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "    print(\"positive papers in train dataset:\",(y_train[:,1]==1).sum())\n",
    "    print(\"positive papers in test dataset:\",(y_val[:,1]==1).sum())\n",
    "\n",
    "    \n",
    "    param_grid = {'model':['bidirectional','backward'], 'optimizer':['rmsprop','adam'],'neurons':[2,5,10], \n",
    "                  'epoch': [5, 10], 'dropout':[0,0.1,0.2,0.3,0.4,0.5,0.7]}\n",
    "    grid = ParameterGrid(param_grid)\n",
    "    \n",
    "    t = PrettyTable(['Model', 'Neurons','Optimizer','Epoch','Dropout','Threshhold','TN', 'FP', 'FN', 'TP','Loss','RunTime (sec)'])\n",
    "    \n",
    "    \n",
    "    results = list()\n",
    "    for params in grid:\n",
    "        \n",
    "        print('model:',params['model'],'optimizer:',params['optimizer'], 'neurons:',params['neurons'],\n",
    "              'epoch:',params['epoch'],' , dropout:',params['dropout'],)\n",
    "        result = run_model(params['model'],params['neurons'],params['optimizer'], params['epoch'], params['dropout'])\n",
    "        results.add(result)\n",
    "        t.add_row(result)\n",
    "        print(t) \n",
    "        \n",
    "    return results  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4061, 1000) , x_val shape: (1016, 1000)\n",
      "y_train shape: (4061, 2) , y_val shape: (1016, 2)\n",
      "positive papers in train dataset: 32\n",
      "positive papers in test dataset: 8\n",
      "epoch: 1  , dropout: 0\n",
      "lstm_bidirectional\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 20)                24880     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               2688      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 6,027,826\n",
      "Trainable params: 27,826\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-0dbc1aa44ad2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## train model on 4000 datapoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults4000\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-93baead5bb51>\u001b[0m in \u001b[0;36mgrid_search_lstm\u001b[1;34m(validation_set, added_positives)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' , dropout:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dropout'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dropout'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-93baead5bb51>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(model_name, epoch, dropout)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-cfb06e382548>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, epoch_no)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         )  \n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\p.zahedi\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## train model on 4000 datapoints\n",
    "results4000 = grid_search_lstm(0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## train model on 500 datapoints\n",
    "results500 = grid_search_lstm(0.9, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## train model on 100 datapoints\n",
    "results100 = grid_search_lstm(0.98, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## train model on 50 datapoints\n",
    "results50 = grid_search_lstm(0.99, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## train model on 20 datapoints\n",
    "results20 = grid_search_lstm(0.996, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
